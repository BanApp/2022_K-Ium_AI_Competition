{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "DKSJP4YQDzVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TFBertForSequenceClassification\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "from transformers import TextClassificationPipeline"
      ],
      "metadata": {
        "id": "vLAItuO7D2af"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "PsXh_P9UDIBE"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/model')\n",
        "model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/model').to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/TrainSet _1st.csv',encoding=\"utf-8\")\n",
        "\n",
        "for i in range(len(data)):\n",
        "  data['Findings'][i] = str(data['Findings'][i]) + str(data['Conclusion\\n'][i]) \n",
        "\n",
        "data_shuffled = data.sample(frac=1).reset_index(drop=True)\n",
        "test = data_shuffled[:]\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "A2spVm6AKH0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in test.Findings]\n",
        "\n",
        "# 라벨 데이터\n",
        "labels = test['AcuteInfarction'].values\n",
        "\n",
        "# Word 토크나이저 토큰화\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "# 시퀀스 설정 및 정수 인덱스 변환 & 패딩\n",
        "MAX_LEN = 509\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# 어텐션 마스크\n",
        "attention_masks = []\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "    \n",
        "# 파이토치 텐서로 변환\n",
        "test_inputs = torch.tensor(input_ids)\n",
        "test_labels = torch.tensor(labels)\n",
        "test_masks = torch.tensor(attention_masks)\n",
        "\n",
        "# 배치 사이즈 설정 및 데이터 설정\n",
        "batch_size = 16\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "ishGhNJ2Ko12"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkqsLIHZL0ai",
        "outputId": "c21df5cd-e222-4936-802b-ab23d0610c3b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "    \n",
        "# 시간 표시 함수\n",
        "def format_time(elapsed):\n",
        "\n",
        "    # 반올림\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # hh:mm:ss으로 형태 변경\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "#시작 시간 설정\n",
        "t0 = time.time()\n",
        "\n",
        "# 평가모드로 변경\n",
        "model.eval()\n",
        "\n",
        "# 변수 초기화\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "for step, batch in enumerate(test_dataloader):\n",
        "    # 경과 정보 표시\n",
        "    if step % 100 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
        "\n",
        "    # 배치를 GPU에 넣음\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # 배치에서 데이터 추출\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "    \n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(\"\")\n",
        "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"Test took: {:}\".format(format_time(time.time() - t0)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0grQyvakLsO9",
        "outputId": "2e838383-7e40-41fb-bdb4-7de083598d44"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   100  of    387.    Elapsed: 0:01:01.\n",
            "  Batch   200  of    387.    Elapsed: 0:02:01.\n",
            "  Batch   300  of    387.    Elapsed: 0:03:02.\n",
            "\n",
            "Accuracy: 0.99\n",
            "Test took: 0:03:55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "#nlp_sentence_classif = pipeline('sentiment-analysis',model=model, tokenizer=tokenizer, device=0)\n",
        "#https://raki-1203.github.io/boostcamp_ai_tech/week_9/03.-single-sentence-classification-based-BERT-train/\n",
        "\n",
        "#입력 데이터에 대한 0,1 분류 결과 출력\n",
        "def sentences_predict(sent):\n",
        "    model.eval()\n",
        "    tokenized_sent = tokenizer(\n",
        "            sent,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128\n",
        "    )\n",
        "    tokenized_sent.to(device)\n",
        "    \n",
        "    with torch.no_grad():# 그라디엔트 계산 비활성화\n",
        "        outputs = model(\n",
        "            input_ids=tokenized_sent['input_ids'],\n",
        "            attention_mask=tokenized_sent['attention_mask'],\n",
        "            token_type_ids=tokenized_sent['token_type_ids']\n",
        "            )\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits)\n",
        "    return result\n",
        "\n",
        "\n",
        "x = data.Findings\n",
        "\n",
        "ans = []\n",
        "for i in range(len(data.Findings)):\n",
        "  ans.append(sentences_predict(x[i]))\n",
        "\n",
        "ck = 0\n",
        "rc = []\n",
        "\n",
        "for i in range(len(ans)):\n",
        "  if ans[i] == data.AcuteInfarction[i]:\n",
        "    ck +=1\n",
        "  else:\n",
        "    rc.append(i)\n",
        "      \n",
        "u = ck / len(ans) \n",
        "print(round(u,9)) #정확도\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaM-voYPX9rz",
        "outputId": "1b1e65a7-3d4e-4be5-f920-8e044d8220a9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.984975767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rc) #오류 데이터들\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-_wkPSLPuAM",
        "outputId": "6da801a4-1696-4701-98fa-8ddffd665935"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[81, 230, 238, 259, 265, 282, 294, 410, 442, 451, 479, 550, 558, 615, 817, 865, 902, 1030, 1113, 1192, 1426, 1508, 1546, 1622, 1662, 1786, 1829, 1909, 1957, 1972, 2014, 2099, 2306, 2327, 2368, 2394, 2531, 2569, 2634, 2844, 2858, 2938, 2948, 3029, 3104, 3150, 3168, 3253, 3269, 3413, 3583, 3632, 3698, 3735, 3767, 3788, 3793, 3812, 3937, 3957, 3969, 3980, 3982, 4031, 4079, 4142, 4144, 4175, 4255, 4307, 4391, 4430, 4482, 4529, 4629, 4675, 4720, 4723, 4900, 4984, 5056, 5106, 5193, 5361, 5390, 5649, 5661, 5826, 5920, 6004, 6048, 6109, 6115]\n"
          ]
        }
      ]
    }
  ]
}